---
title: 爬虫部署
top: false
cover: false
toc: false
mathjax: true
tags:
  - scrapyd
  - 部署
  - 爬虫
  - 项目
categories:
  - 项目
summary: 爬虫部署API服务
abbrlink: b77d
date: 2021-05-17 13:15:19
password:
keywords:
description:
---
# 爬虫部署API服务

### 1. 部署流程

```bash
# 安装scrapyd框架，用于部署scrapy框架的服务，可将其转化为API接口的形式去运行以及配置参数
$ pip install scrapyd
# 在项目根目录下先运行scrapyd，单独运行在一个终端中，这里将显示访问接口的信息
$ scrapyd
# 更改scrapy.cfg中[deploy]，更改为[deploy:first_deploy]，启动服务后可以看见部署服务的名称，first_deploy         http://localhost:6800
$ scrapyd-deploy -l
# 再执行list语句，显示出scrapy的spider名称MyMooc，即为打包成功
$ scrapy list
# 开始项目部署，这个命令会将项目打包成egg文件，会在根目录下创建一个setup.py，会返回一个addversion.json，出现即为部署成功
$ scrapyd-deploy (部署名称)first_deploy -p (项目名称)MyMooc
# 运行一次代码，
$ curl http://localhost:6800/schedule.json -d project=first_deploy -d spider=MyMooc

# 定义变量
# 加入可选参数，这样在运行scrapy crawl MyMooc在后面就可以添加-a设置参数，
# scrapy crawl MyMooc -a p=xxx -a test=xxx
# 这样就解决的scrapyd中可以设置参数的问题，非常nice，perfect！！！
# 在后面的程序中就可以使用self.p，self.test引入参数
def __init__(self, p=None, test=None, *args, **kwargs):
  super(MySpider, self).__init__(*args, **kwargs)
  self.p = p
  self.test = test
```

### 2. 利用postman加入参数测试接口

- **运行scrapyd，在127.0.0.1:6800访问**

- **查看scrapyd服务器运行状态，get方法，（scrapyd_status）**

- ```bash
  curl http://127.0.0.1:6800/daemonstatus.json
  #返回
  {
      "node_name": "MM-202104301802",
      "status": "ok",
      "pending": 0,
      "running": 0,
      "finished": 0
  }
  ```

- **获取scrapyd服务器上已经发布的工程列表，get方法，（scrapyd_project）**

- ```bash
  curl http://127.0.0.1:6800/listprojects.json
  {
      "node_name": "MM-202104301802",
      "status": "ok",
      "projects": [
          "MyMooc",
          "default"
      ]
  }
  ```

- **获取scrapyd服务器上名为MyMooc的工程下的爬虫清单，get方法，（project_spider）**

- ```bash
  curl http://127.0.0.1:6800/listspiders.json?project=mooc
  {
      "node_name": "MM-202104301802",
      "status": "ok",
      "spiders": [
          "MyMooc"
      ]
  }
  spider是爬虫文件中spiders文件夹中那个爬虫py文件的name
  
  ```

- **获取scrapyd服务器上名为MyMooc的工程下的各爬虫的版本，get方法，（project_all_version）**

- ```bash
  curl http://127.0.0.1:6800/listversions.json?project=MyMooc
  {
      "node_name": "MM-202104301802",
      "status": "ok",
      "versions": [
          "1620801044",
          "1620819352"
      ]
  }
  ```

- **获取scrapyd服务器上的所有任务清单，包括已结束，正在运行的，准备启动的。get方法，（scrapyd_all_jobs）**

- ```bash
  curl http://127.0.0.1:6800/listjobs.json?project=mooc
  {
      "node_name": "MM-202104301802",
      "status": "ok",
      "pending": [],
      "running": [],
      "finished": []
  }
  ```

- **启动 scrapyd服务器上myproject工程下的myspider爬虫，使myspider立刻开始运行 ，注意必须以post方式，（run_project_spider）**

- ```bash
  http://127.0.0.1:6800/schedule.json -d project=myproject -d spider=myspider
  http://127.0.0.1:6800/schedule.json -d project=5.13.01 -d spider=MyMooc
  {"node_name": "MM-202104301802", "status": "ok", "project": "MyMooc", "version": "1620885389", "spiders": 1}
  
  ```

- 删除scrapyd服务器上myproject的工程下的版本名为version的爬虫 ，注意必须以post方式

- ```bash
  http://127.0.0.1:6800/delversion.json -d project=myproject -d version=r99'
  
  ```

- **删除scrapyd服务器上myproject工程，注意该命令会自动删除该工程下所有的spider，注意必须以post方式**

- ```bash
  http://127.0.0.1:6800/delproject.json -d project=myproject
  {
      "node_name": "MM-202104301802",
      "status": "ok"
  }
  ```

- ```bash
  curl http://127.0.0.1:6800/schedule.json -d project=MyMooc -d spider=MyMooc -d p='["123"]'
  ```

  

