---
title: SimCSE（十）
top: false
cover: false
toc: false
mathjax: true
tags:
  - 论文
  - 研究生
  - 知识图谱
  - SimCSE
categories:
  - 论文
summary: SimCSE归纳总结
abbrlink: 9be8
date: 2021-05-06 20:47:14
password:
keywords:
description:
---
# SimCSE归纳总结

1. SimCSE，运用了dropout的方法替换了原有的数据增强方法，将输入两次dropout产生的结果当作学习的正例

   - 为什么要进行数据增强：在深度学习中，一般要求样本的数量要充足，样本数量越多，训练出来的模型效果越好，模型的泛化能力越强。但是实际中，样本数量不足或者样本质量不够好，这就要对样本做数据增强，来提高样本质量

   - 传统数据增强方法：
     - 图片：如果数据集都是图片类，可以运用翻转，旋转，缩放，剪裁，平移，添加噪声
     - 文本：数据是文本类的，可以使用回译，替换同义词，随机插入一个词语，随机交换词语，p概率删除一个词语

2. 用**自然语言推理NLI**的数据用于有监督的对比学习：自然语言推理研究一个假设是否可以从一个前提中推断出来，假设和前提都是文本序列。换句话说，自然语言推理决定了一对文本序列之间的逻辑关系。这种关系通常分为三类：       

   - 蕴涵：假设可以从前提中推断出来。       

   - 矛盾：假设的否定可以从前提推断出来。       

   - 中立：所有其情况。       

自然语言推理也被称为识别文本蕴涵任务。例如，下面的一对会被标记为蕴涵，因为假设中的“示爱”可以从前提中的“拥抱”中推断出来。       

前提：两个女人互相拥抱。       

假设：两个女人在表达爱意。       

下面是一个矛盾的例子，因为“运行代码”表示“不睡觉”而不是“睡眠”。       

前提：一个男人正在运行一个代码示例

假设：这个人正在睡觉。       

第三个例子显示了一种中立关系，因为“为演出”的事实不能推断出“著名”和“不出名”。       

前提：音乐家正在表演。       

假设：音乐家是有名的。       

3. 有监督学习(supervised learning)和无监督学习(unsupervised learning)

   - supervised learning：通过已有的样本去群里安处一个最优模型，再利用这个模型将就可以将其他的未知数据输入，得到已知的分类。就像小时候父母告诉我们这个是苹果，这个是狗，这就是已构建好的模型和函数以及分类，当身边没有大人时，当我们看到类似的东西时，也可以分辨出其他的水果以及动物。经典有监督学习（KNN，SVM以后学习一下）
   - unsupervised learning：我们没有任何训练样本，直接利用已有数据进行建模。比如我们去参观一个画展，我们完全对艺术一无所知，但是欣赏完多幅作品之后，我们也能把它们分成不同的派别（比如哪些更朦胧一点，哪些更写实一些，即使我们不知道什么叫做朦胧派，什么叫做写实派，但是至少我们能把他们分为两个类）。无监督学习里典型的例子就是聚类了。聚类的目的在于把相似的东西聚在一起，而我们并不关心这一类是什么。因此，一个聚类算法通常只需要知道如何计算相似度就可以开始工作了。
   - 有训练样本就用有监督学习，没有训练样本肯定用不了有监督学习。但现实中，在没有训练样本的情况下，我们可以通过人工标注一些样本，然后去用有监督的方法去做

4. 泛化误差：对于深度学习或机器学习模型而言，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以对未知数据集（测试集）有很好的拟合结果（泛化能力），所产生的测试误差被称为**泛化误差**。

   - 训练刚开始的时候，模型还在学习过程中，处于欠拟合区域。随着训练的进行，训练误差和测试误差都下降。在到达一个临界点之后，训练集的误差下降，测试集的误差上升了，这个时候就进入了过拟合区域——由于训练出来的网络**过度拟合了训练集**，对训练集以外的数据却不work。

   <img src="拟合对比.png" alt="拟合对比"  />

   - 具体过拟合欠拟合等解决办法见http://blog.bangbangbangbang.top/posts/5c43.html/

5. Dropout：是一种防止过拟合的方法，处理过后也可作为新数据加入数据集中，增加数据集的多样性。

   - 在每次训练中，减少每个相关元素之间一半的关联，就可以明显减少过拟合现象。通过这种方法，可以减少相关元素之间的相互作用，可以使模型的泛化能力更强，因为不会过于依赖某些局部特征

   <img src="dropout.jpeg" alt="dropout" style="zoom:50%;" />

6. 对比表示学习（Contrastive Representation Learning）
   - 主要是要**构建正样本$x^+$和负样本$x^-$**，正例就是与x相似的数据，负例就是与x不同的数据
   - 在论文[Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere](http://blog.bangbangbangbang.top/posts/3b4c.html/ICML2020.pdf)中指出了对比学习的两个指标
   - **Alignment**对齐性，针对正例：计算正例对之间的向量距离的期望，越相似的正例对之间的alignment程度越高，距离越小。
   - **Uniformity**均匀性，针对负例：所有数据的向量分布越均匀，保留的信息越多。最极端的反例，所有数据都映射到单位超球面的同一点上，这就极度违背了uniformity原则，这代表数据的所有信息都没了，只有一个点的信息了。负例对之间，距离越远愈好
   - 这篇论文也采用这两个指标来衡量生成的句子向量，alignment和uniformity越低，向量的质量越高，在STS任务上的Spearman相关系数越高。spearman是一种相关系数方法

7. 论文中的SimCSE方法

   - Unsupervised SimCSE：引入dropout给输入加入噪声，假设加入噪声后的输入与原始输入在语义空间上距离相近。然后将相同的语句输入两次给预训练模型，得到一个正例对。因为dropout每次是以p概率隐去一半的相关性，所以相同输入所传出的值每次都是不同的，可以用作正例。

   - supervised SimCSE：利用标注数据来构造对比学习的正负例子，最终选择NLI作为数据集

8. 熵entropy
   - 熵**(Entropy)**：可以表示一个事件A的自信息量，也就是A包含多少信息。
   - KL散度**(Kullback-Leibler Divergence)**：可以用来表示从事件A的角度来看，事件B有多大不同。
   - 交叉熵**(Cross Entropy)**：可以用来表示从事件A的角度来看，如何描述事件B。
   - **KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价**。