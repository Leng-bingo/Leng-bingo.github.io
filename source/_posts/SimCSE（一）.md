---
title: SimCSE（一）
top: false
cover: false
toc: false
mathjax: true
tags:
  - 论文
  - 研究生
  - 知识图谱
  - SimCSE
categories:
  - 论文
summary: SimCSE简介
abbrlink: 5c43
date: 2021-04-26 17:11:43
password:
keywords:
description:
---
## Simple Contrastive Learning of Sentence Embeddings（一）

SimCSE的全称为 *Simple Contrastice  Learning of Sentence Embeddings*。**Sim代表Simple，就是简单**。

他简单的运用了 **<font color = "red">dropout</font>** 的方法替换了传统的数据增强方法，将同一个输入dropout两次作为对比学习的正例，而且效果甚好。

------------

**<font color = "red">红色：</font>** 名词第一次出现

**<font color = "#6638F0">紫色：</font>** 解释专有名词

**<font color = "#F78AE0">粉色：</font>** 原因以及解决方法

-------

- **<font color = "#6638F0">拟合：</font>** 拟合就是把平面上一系列的点，用一条光滑的曲线连接起来。因为这条曲线有无数种可能，从而有各种拟合方法。拟合的曲线一般可以用函数表示，根据这个函数的不同有不同的拟合名字。
- **<font color = "#6638F0">泛化误差：</font>** 对于深度学习或机器学习模型而言，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以对未知数据集（测试集）有很好的拟合结果（泛化能力），所产生的测试误差被称为**泛化误差**。

<img src="拟合图片.jpeg" alt="拟合图片" style="zoom: 67%;" />

- 训练刚开始的时候，模型还在学习过程中，处于欠拟合区域。随着训练的进行，训练误差和测试误差都下降。在到达一个临界点之后，训练集的误差下降，测试集的误差上升了，这个时候就进入了过拟合区域——由于训练出来的网络**过度拟合了训练集**，对训练集以外的数据却不work。

<img src="拟合对比.png" alt="拟合对比"  />

- **<font color = "#6638F0">欠拟合：</font>** 欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。
  - **<font color = "#F78AE0">如何解决欠拟合：</font>** 欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中 **<font color = "red">增加特征</font>** ，这些都是很好解决欠拟合的方法。
  - **<font color = "#6638F0">增加特征：</font>** 添加高次多项式，使模型泛化能力更强
- **<font color = "#6638F0">过拟合：</font>** 过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，**模型在训练集上表现很好，但在测试集上却表现很差**。模型对训练集"死记硬背"（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，**泛化能力差**。
  - **<font color = "#F78AE0">造成原因：</font>** **训练数据集样本单一，样本不足**。所以训练样本要尽可能的全面，覆盖所有的数据类型。**训练数据中噪声干扰过大**。噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系。**模型过于复杂。**模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。
  - **<font color = "#F78AE0">防止过拟合：</font>** 要想解决过拟合问题，就要显著减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。我们可以使用正则化（Regularization）方法。**正则化是指修改学习算法，使其降低泛化误差而非训练误差**。
  - 常用的正则化方法根据具体的使用策略不同可分为：（1）直接提供正则化约束的参数正则化方法，如L1/L2正则化；（2）通过工程上的技巧来实现更低泛化误差的方法，如提前终止(Early stopping)和 **<font color = "red">Dropout</font>** ；（3）不直接提供约束的隐式正则化方法，如数据增强等。 **目前主要学习Dropout方法** 。

-----------

### 1. Dropout简介

#### 1.1 Dropout出现的原因

在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。

过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。

综上所述，训练深度神经网络的时候，总是会遇到两大缺点：

（1）容易过拟合

（2）费时

Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到 **<font color = "red">正则化</font>** 的效果。

- **<font color = "#6638F0">正则化</font>** （regularization）是指为解决适定性问题或过拟合而加入额外信息的过程。

#### 1.2 什么是Dropout

Dropout可以作为训练深度神经网络的 **<font color = "red">一种trick</font>** 供选择。在每个训练批次中，通过 **忽略一半** 的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。

Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，如图所示。

<img src="dropout.jpeg" alt="dropout" style="zoom:50%;" />

### 2. Dropout工作流程及使用

输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：

1. 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）

> 还没有搞懂，下次再看到dropout时，再回来对应学习

2. 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。

3. 然后继续重复这一过程：

- 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）
- 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。
- 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。
